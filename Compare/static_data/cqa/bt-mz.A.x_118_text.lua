_cqa_text_report = {
  paths = {
    {
      hint = {
        {
          details = "These instructions generate more than one micro-operation and only one of them can be decoded during a cycle and the extra micro-operations increase pressure on execution units.\n - ADD: 1 occurrences\n - VEXTRACTF128: 5 occurrences\n - VINSERTF128: 10 occurrences\n",
          title = "Complex instructions",
          txt = "Detected COMPLEX INSTRUCTIONS.\n",
        },
        {
          workaround = "Use vector aligned instructions:\n 1) The GNU Fortran compiler does not support allocation alignment and does not feature directives to benefit from aligned data.\n 2) Use another compiler or locally use C code (for instance via libraries)\n",
          details = " - VEXTRACTF128: 5 occurrences\n - VINSERTF128: 10 occurrences\n - VMOVUPS: 5 occurrences\n",
          title = "Vector unaligned load/store instructions",
          txt = "Detected 20 optimal vector unaligned load/store instructions.\n",
        },
        {
          title = "Type of elements and instruction set",
          txt = "30 SSE or AVX instructions are processing arithmetic or math operations on double precision FP elements in scalar mode (one at a time).\n5 AVX instructions are processing arithmetic or math operations on double precision FP elements in vector mode (four at a time).\n",
        },
        {
          title = "Matching between your loop (in the source code) and the binary loop",
          txt = "The binary loop is composed of 50 FP arithmetical operations:\n - 50: addition or subtraction\nThe binary loop is loading 920 bytes (115 double precision FP elements).\nThe binary loop is storing 404 bytes (50 double precision FP elements).",
        },
        {
          title = "Arithmetic intensity",
          txt = "Arithmetic intensity is 0.04 FP operations per loaded or stored byte.",
        },
      },
      expert = {
        {
          title = "General properties",
          txt = "nb instructions    : 219\nnb uops            : 235\nloop length        : 1087\nused x86 registers : 15\nused mmx registers : 0\nused xmm registers : 16\nused ymm registers : 16\nused zmm registers : 0\nnb stack references: 18\n",
        },
        {
          title = "Front-end",
          txt = "MACRO FUSION NOT POSSIBLE\nFIT IN UOP CACHE\nmicro-operation queue: 58.75 cycles\nfront end            : 58.75 cycles\n",
        },
        {
          title = "Back-end",
          txt = "       | P0    | P1    | P2    | P3    | P4    | P5\n------------------------------------------------------\nuops   | 43.33 | 43.33 | 70.00 | 70.00 | 41.00 | 43.33\ncycles | 43.33 | 43.33 | 70.00 | 70.00 | 41.00 | 43.33\n\nCycles executing div or sqrt instructions: NA\n",
        },
        {
          title = "Cycles summary",
          txt = "Front-end : 58.75\nDispatch  : 70.00\nOverall L1: 70.00\n",
        },
        {
          title = "Vectorization ratios",
          txt = "INT\nall    : 0%\nload   : 0%\nstore  : NA (no store vectorizable/vectorized instructions)\nmul    : NA (no mul vectorizable/vectorized instructions)\nadd-sub: 0%\nfma    : NA (no fma vectorizable/vectorized instructions)\nother  : 0%\nFP\nall     : 28%\nload    : 25%\nstore   : 25%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : 14%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 100%\nINT+FP\nall     : 24%\nload    : 22%\nstore   : 25%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : 13%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 43%\n",
        },
        {
          title = "Vector efficiency ratios",
          txt = "INT\nall    : 35%\nload   : 28%\nstore  : NA (no store vectorizable/vectorized instructions)\nmul    : NA (no mul vectorizable/vectorized instructions)\nadd-sub: 25%\nfma    : NA (no fma vectorizable/vectorized instructions)\nother  : 36%\nFP\nall     : 34%\nload    : 31%\nstore   : 31%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : 35%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 50%\nINT+FP\nall     : 34%\nload    : 31%\nstore   : 31%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : 35%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 42%\n",
        },
        {
          title = "Cycles and memory resources usage",
          txt = "Assuming all data fit into the L1 cache, each iteration of the binary loop takes 70.00 cycles. At this rate:\n - 41% of peak load performance is reached (13.14 out of 32.00 bytes loaded per cycle (GB/s @ 1GHz))\n - 36% of peak store performance is reached (5.77 out of 16.00 bytes stored per cycle (GB/s @ 1GHz))\n",
        },
        {
          title = "Front-end bottlenecks",
          txt = "Found no such bottlenecks.",
        },
        {
          title = "ASM code",
          txt = "In the binary file, the address of the loop is: 1e020\n\nInstruction                               | Nb FU | P0   | P1   | P2   | P3   | P4 | P5   | Latency | Recip. throughput\n-----------------------------------------------------------------------------------------------------------------------\nMOV 0x70(%RSP),%RDI                       | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 2       | 0.50\nMOVSXD %R9D,%R8                           | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nMOV 0x68(%RSP),%RDX                       | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 2       | 0.50\nMOV 0x58(%RSP),%RAX                       | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 2       | 0.50\nIMUL %R14,%RDI                            | 1     | 0    | 1    | 0    | 0    | 0  | 0    | 3       | 1\nIMUL %R8,%RDX                             | 1     | 0    | 1    | 0    | 0    | 0  | 0    | 3       | 1\nADD 0x78(%RSP),%RDI                       | 1     | 0.33 | 0.33 | 0.50 | 0.50 | 0  | 0.33 | 1       | 0.50\nIMUL %R14,%RAX                            | 1     | 0    | 1    | 0    | 0    | 0  | 0    | 3       | 1\nIMUL 0x50(%RSP),%R8                       | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nADD 0x60(%RSP),%RAX                       | 1     | 0.33 | 0.33 | 0.50 | 0.50 | 0  | 0.33 | 1       | 0.50\nADD %RDX,%RDI                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nLEA 0x6(%RDI),%RBX                        | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nADD %RAX,%R8                              | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nLEA 0x30(,%R8,8),%RCX                     | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nLEA 0xd0(,%RDI,8),%RAX                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nLEA (%R11,%RCX,1),%RSI                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nLEA -0xa0(%R10,%RAX,1),%RDX               | 1     | 0    | 1    | 0    | 0    | 0  | 0    | 3       | 1\nLEA 0xa0(%R11,%RCX,1),%RCX                | 1     | 0    | 1    | 0    | 0    | 0  | 0    | 3       | 1\nCMP %RCX,%RDX                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nSETAE %CL                                 | 1     | 0.50 | 0    | 0    | 0    | 0  | 0.50 | 1       | 0.50\nADD %R10,%RAX                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nCMP %RAX,%RSI                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nSETAE %AL                                 | 1     | 0.50 | 0    | 0    | 0    | 0  | 0.50 | 1       | 0.50\nOR %AL,%CL                                | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nJE 1e52f <add_._omp_fn.0+0x66f>           | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nCMPL $0x2,0x44(%RSP)                      | 1     | 0.33 | 0.33 | 0.50 | 0.50 | 0  | 0.33 | 1       | 0.50\nJBE 1e52f <add_._omp_fn.0+0x66f>          | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nLEA (%RDX,%R15,1),%RBX                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nMOV %RDX,%RAX                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nMOV %RSI,%RCX                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nTEST $0x20,%R15B                          | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nJE 1e1a8 <add_._omp_fn.0+0x2e8>           | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nVMOVUPD 0x20(%RSI),%XMM2                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVINSERTF128 $0x1,0x30(%RSI),%YMM2,%YMM0   | 2     | 0    | 0    | 0.50 | 0.50 | 0  | 1    | 2       | 1\nADD $0xa0,%RAX                            | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nADD $0xa0,%RCX                            | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nVMOVUPD 0x40(%RSI),%XMM7                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVINSERTF128 $0x1,0x50(%RSI),%YMM7,%YMM8   | 2     | 0    | 0    | 0.50 | 0.50 | 0  | 1    | 2       | 1\nVMOVUPD 0x60(%RSI),%XMM12                 | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVINSERTF128 $0x1,0x70(%RSI),%YMM12,%YMM13 | 2     | 0    | 0    | 0.50 | 0.50 | 0  | 1    | 2       | 1\nVMOVUPD 0x20(%RDX),%XMM1                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVINSERTF128 $0x1,0x30(%RDX),%YMM1,%YMM3   | 2     | 0    | 0    | 0.50 | 0.50 | 0  | 1    | 2       | 1\nVADDPD %YMM0,%YMM3,%YMM6                  | 1     | 0    | 1    | 0    | 0    | 0  | 0    | 3       | 1\nVMOVUPD 0x40(%RDX),%XMM4                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVINSERTF128 $0x1,0x50(%RDX),%YMM4,%YMM5   | 2     | 0    | 0    | 0.50 | 0.50 | 0  | 1    | 2       | 1\nVADDPD %YMM8,%YMM5,%YMM9                  | 1     | 0    | 1    | 0    | 0    | 0  | 0    | 3       | 1\nVMOVUPD 0x80(%RSI),%XMM3                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVINSERTF128 $0x1,0x90(%RSI),%YMM3,%YMM2   | 2     | 0    | 0    | 0.50 | 0.50 | 0  | 1    | 2       | 1\nVMOVUPD (%RSI),%XMM5                      | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVINSERTF128 $0x1,0x10(%RSI),%YMM5,%YMM7   | 2     | 0    | 0    | 0.50 | 0.50 | 0  | 1    | 2       | 1\nVMOVUPS %XMM6,0x20(%RDX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVEXTRACTF128 $0x1,%YMM6,0x30(%RDX)        | 2     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 2       | 1\nVMOVUPD (%RDX),%XMM4                      | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVINSERTF128 $0x1,0x10(%RDX),%YMM4,%YMM8   | 2     | 0    | 0    | 0.50 | 0.50 | 0  | 1    | 2       | 1\nVMOVUPS %XMM9,0x40(%RDX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVEXTRACTF128 $0x1,%YMM9,0x50(%RDX)        | 2     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 2       | 1\nVMOVUPD 0x60(%RDX),%XMM10                 | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVINSERTF128 $0x1,0x70(%RDX),%YMM10,%YMM11 | 2     | 0    | 0    | 0.50 | 0.50 | 0  | 1    | 2       | 1\nVADDPD %YMM7,%YMM8,%YMM10                 | 1     | 0    | 1    | 0    | 0    | 0  | 0    | 3       | 1\nVADDPD %YMM13,%YMM11,%YMM14               | 1     | 0    | 1    | 0    | 0    | 0  | 0    | 3       | 1\nVMOVUPD 0x80(%RDX),%XMM15                 | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVINSERTF128 $0x1,0x90(%RDX),%YMM15,%YMM1  | 2     | 0    | 0    | 0.50 | 0.50 | 0  | 1    | 2       | 1\nVADDPD %YMM2,%YMM1,%YMM0                  | 1     | 0    | 1    | 0    | 0    | 0  | 0    | 3       | 1\nVMOVUPS %XMM10,(%RDX)                     | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVEXTRACTF128 $0x1,%YMM10,0x10(%RDX)       | 2     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 2       | 1\nVMOVUPS %XMM14,0x60(%RDX)                 | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVEXTRACTF128 $0x1,%YMM14,0x70(%RDX)       | 2     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 2       | 1\nVMOVUPS %XMM0,0x80(%RDX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVEXTRACTF128 $0x1,%YMM0,0x90(%RDX)        | 2     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 2       | 1\nCMP %RBX,%RAX                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nJE 1e3be <add_._omp_fn.0+0x4fe>           | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nNOPL (%RAX)                               | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0       | 0.25\nCMP 0x40(%RSP),%R13D                      | 1     | 0.33 | 0.33 | 0.50 | 0.50 | 0  | 0.33 | 1       | 0.50\nJE 1e4fe <add_._omp_fn.0+0x63e>           | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nMOV 0x30(%RSP),%RDX                       | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 2       | 0.50\nLEA (%RDI,%RDX,1),%RAX                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nLEA (%RDX,%R8,1),%RBX                     | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nLEA (%R10,%RAX,8),%RCX                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nLEA (%R11,%RBX,8),%RSI                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nVMOVSD 0x8(%RCX),%XMM10                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVADDSD 0x8(%RSI),%XMM10,%XMM11            | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD 0x10(%RCX),%XMM14                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD 0x18(%RCX),%XMM1                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD %XMM11,0x8(%RCX)                   | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x10(%RSI),%XMM14,%XMM15           | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD 0x20(%RCX),%XMM0                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD 0x28(%RCX),%XMM2                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD %XMM15,0x10(%RCX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x18(%RSI),%XMM1,%XMM3             | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM3,0x18(%RCX)                   | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x20(%RSI),%XMM0,%XMM13            | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM13,0x20(%RCX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x28(%RSI),%XMM2,%XMM4             | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM4,0x28(%RCX)                   | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nCMP 0x3c(%RSP),%R13D                      | 1     | 0.33 | 0.33 | 0.50 | 0.50 | 0  | 0.33 | 1       | 0.50\nJL 1e4fe <add_._omp_fn.0+0x63e>           | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nMOV 0x28(%RSP),%RDX                       | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 2       | 0.50\nLEA (%RDI,%RDX,1),%RAX                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nLEA (%R8,%RDX,1),%RBX                     | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nLEA (%R10,%RAX,8),%RCX                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nLEA (%R11,%RBX,8),%RSI                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nVMOVSD 0x8(%RCX),%XMM8                    | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVADDSD 0x8(%RSI),%XMM8,%XMM5              | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD 0x10(%RCX),%XMM7                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD 0x18(%RCX),%XMM6                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD %XMM5,0x8(%RCX)                    | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x10(%RSI),%XMM7,%XMM9             | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD 0x20(%RCX),%XMM10                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD 0x28(%RCX),%XMM14                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD %XMM9,0x10(%RCX)                   | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x18(%RSI),%XMM6,%XMM12            | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM12,0x18(%RCX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x20(%RSI),%XMM10,%XMM11           | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM11,0x20(%RCX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x28(%RSI),%XMM14,%XMM15           | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM15,0x28(%RCX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nCMP 0x38(%RSP),%R13D                      | 1     | 0.33 | 0.33 | 0.50 | 0.50 | 0  | 0.33 | 1       | 0.50\nJL 1e4fe <add_._omp_fn.0+0x63e>           | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nMOV 0x20(%RSP),%RDX                       | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 2       | 0.50\nADD %RDX,%RDI                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nADD %RDX,%R8                              | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nLEA (%R10,%RDI,8),%RDI                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nLEA (%R11,%R8,8),%R8                      | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nVMOVSD 0x8(%RDI),%XMM1                    | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVADDSD 0x8(%R8),%XMM1,%XMM3               | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD 0x10(%RDI),%XMM0                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD 0x18(%RDI),%XMM2                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD %XMM3,0x8(%RDI)                    | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x10(%R8),%XMM0,%XMM13             | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD 0x20(%RDI),%XMM8                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD 0x28(%RDI),%XMM7                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD %XMM13,0x10(%RDI)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x18(%R8),%XMM2,%XMM4              | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM4,0x18(%RDI)                   | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x20(%R8),%XMM8,%XMM5              | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM5,0x20(%RDI)                   | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x28(%R8),%XMM7,%XMM9              | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM9,0x28(%RDI)                   | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nCMP 0x4c(%RSP),%R12D                      | 1     | 0.33 | 0.33 | 0.50 | 0.50 | 0  | 0.33 | 1       | 0.50\nJE 1e51d <add_._omp_fn.0+0x65d>           | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nADD $0x1,%R9D                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nCMP %R9D,0x48(%RSP)                       | 1     | 0.33 | 0.33 | 0.50 | 0.50 | 0  | 0.33 | 1       | 0.50\nJLE 1e7d4 <add_._omp_fn.0+0x914>          | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nADD $0x1,%R12D                            | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nJMP 1e020 <add_._omp_fn.0+0x160>          | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 2\nADD 0x10(%RSP),%RDI                       | 1     | 0.33 | 0.33 | 0.50 | 0.50 | 0  | 0.33 | 1       | 0.50\nLEA (%R10,%RBX,8),%RAX                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nMOV %RSI,%RDX                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nMOV 0x8(%RSP),%RCX                        | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 2       | 0.50\nLEA (%RCX,%RDI,8),%RBX                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nMOV %RBX,%RDI                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nSUB %RAX,%RDI                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nSUB $0x28,%RDI                            | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nSHR $0x3,%RDI                             | 1     | 0.50 | 0    | 0    | 0    | 0  | 0.50 | 1       | 0.50\nADD $0x1,%RDI                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nAND $0x3,%EDI                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nJE 1e66d <add_._omp_fn.0+0x7ad>           | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nCMP $0x1,%RDI                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nJE 1e612 <add_._omp_fn.0+0x752>           | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nCMP $0x2,%RDI                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nJE 1e5c0 <add_._omp_fn.0+0x700>           | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nVMOVSD (%RAX),%XMM6                       | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nADD $0x28,%RDX                            | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nADD $0x28,%RAX                            | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nVADDSD (%RSI),%XMM6,%XMM12                | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD -0x20(%RAX),%XMM10                 | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD -0x18(%RAX),%XMM14                 | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD %XMM12,-0x28(%RAX)                 | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x8(%RSI),%XMM10,%XMM11            | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD -0x10(%RAX),%XMM1                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD -0x8(%RAX),%XMM0                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD %XMM11,-0x20(%RAX)                 | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x10(%RSI),%XMM14,%XMM15           | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM15,-0x18(%RAX)                 | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x18(%RSI),%XMM1,%XMM3             | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM3,-0x10(%RAX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x20(%RSI),%XMM0,%XMM13            | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM13,-0x8(%RAX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVMOVSD (%RAX),%XMM2                       | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nADD $0x28,%RDX                            | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nADD $0x28,%RAX                            | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nVADDSD -0x28(%RDX),%XMM2,%XMM4            | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD -0x20(%RAX),%XMM8                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD -0x18(%RAX),%XMM7                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD %XMM4,-0x28(%RAX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD -0x20(%RDX),%XMM8,%XMM5            | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD -0x10(%RAX),%XMM6                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD -0x8(%RAX),%XMM10                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD %XMM5,-0x20(%RAX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD -0x18(%RDX),%XMM7,%XMM9            | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM9,-0x18(%RAX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD -0x10(%RDX),%XMM6,%XMM12           | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM12,-0x10(%RAX)                 | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD -0x8(%RDX),%XMM10,%XMM11           | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM11,-0x8(%RAX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVMOVSD (%RAX),%XMM14                      | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nADD $0x28,%RAX                            | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nADD $0x28,%RDX                            | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nVADDSD -0x28(%RDX),%XMM14,%XMM15          | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD -0x20(%RAX),%XMM1                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD -0x18(%RAX),%XMM0                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD %XMM15,-0x28(%RAX)                 | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD -0x20(%RDX),%XMM1,%XMM3            | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD -0x10(%RAX),%XMM2                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD -0x8(%RAX),%XMM8                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD %XMM3,-0x20(%RAX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD -0x18(%RDX),%XMM0,%XMM13           | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM13,-0x18(%RAX)                 | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD -0x10(%RDX),%XMM2,%XMM4            | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM4,-0x10(%RAX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD -0x8(%RDX),%XMM8,%XMM5             | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM5,-0x8(%RAX)                   | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nCMP %RBX,%RAX                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nJE 1e4fe <add_._omp_fn.0+0x63e>           | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nJMP 1e4fe <add_._omp_fn.0+0x63e>          | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 2\nADDL $0x1,0x1c(%RSP)                      | 2     | 0.33 | 0.33 | 1    | 1    | 1  | 0.33 | 6       | 1\nMOV $0x1,%R9D                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nMOVSXD 0x1c(%RSP),%R14                    | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 1       | 0.50\nJMP 1e514 <add_._omp_fn.0+0x654>          | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 2\n",
        },
      },
      header = {
        "8% of peak computational performance is used (0.71 out of 8.00 FLOP per cycle (GFLOPS @ 1GHz))",
      },
      brief = {
      },
      gain = {
        {
          workaround = " - Try to reorganize arrays of structures to structures of arrays\n - Consider to permute loops (see vectorization gain report)\n - To reference allocatable arrays, use \"allocatable\" instead of \"pointer\" pointers or qualify them with the \"contiguous\" attribute (Fortran 2008)\n - For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop\n",
          title = "Code clean check",
          txt = "Detected a slowdown caused by scalar integer instructions (typically used for address computation).\nBy removing them, you can lower the cost of an iteration from 70.00 to 61.50 cycles (1.14x speedup).",
        },
        {
          workaround = " - Try another compiler or update/tune your current one:\n  * recompile with fassociative-math (included in Ofast or ffast-math) to extend loop vectorization to FP reductions.\n - Remove inter-iterations dependences from your loop and make it unit-stride:\n  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:\nFortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)\n  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):\ndo i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)\n",
          details = "24% of SSE/AVX instructions are used in vector version (process two or more data elements in vector registers):\n - 22% of SSE/AVX loads are used in vector version.\n - 25% of SSE/AVX stores are used in vector version.\n - 13% of SSE/AVX addition or subtraction instructions are used in vector version.\n - 43% of SSE/AVX instructions that are not load, store, addition, subtraction nor multiply instructions are used in vector version.\nSince your execution units are vector units, only a fully vectorized loop can use their full power.\n",
          title = "Vectorization",
          txt = "Your loop is poorly vectorized.\nOnly 34% of vector register length is used (average across all SSE/AVX instructions).\nBy fully vectorizing your loop, you can lower the cost of an iteration from 70.00 to 34.07 cycles (2.05x speedup).",
        },
        {
          workaround = " - Read less array elements\nAll SSE and/or AVX registers are used:\nin that case, try to relax register pressure by reducing the unroll factor or splitting your loop\n - Write less array elements\n - Provide more information to your compiler:\n  * hardcode the bounds of the corresponding 'for' loop\n",
          title = "Execution units bottlenecks",
          txt = "Performance is limited by:\n - reading data from caches/RAM (load units are a bottleneck)\n - writing data to caches/RAM (the store unit is a bottleneck)\n\nBy removing all these bottlenecks, you can lower the cost of an iteration from 70.00 to 58.75 cycles (1.19x speedup).\n",
        },
      },
      potential = {
      },
    },
  },
  AVG = {
      hint = {
        {
          details = "These instructions generate more than one micro-operation and only one of them can be decoded during a cycle and the extra micro-operations increase pressure on execution units.\n - ADD: 1 occurrences\n - VEXTRACTF128: 5 occurrences\n - VINSERTF128: 10 occurrences\n",
          title = "Complex instructions",
          txt = "Detected COMPLEX INSTRUCTIONS.\n",
        },
        {
          workaround = "Use vector aligned instructions:\n 1) The GNU Fortran compiler does not support allocation alignment and does not feature directives to benefit from aligned data.\n 2) Use another compiler or locally use C code (for instance via libraries)\n",
          details = " - VEXTRACTF128: 5 occurrences\n - VINSERTF128: 10 occurrences\n - VMOVUPS: 5 occurrences\n",
          title = "Vector unaligned load/store instructions",
          txt = "Detected 20 optimal vector unaligned load/store instructions.\n",
        },
        {
          title = "Type of elements and instruction set",
          txt = "30 SSE or AVX instructions are processing arithmetic or math operations on double precision FP elements in scalar mode (one at a time).\n5 AVX instructions are processing arithmetic or math operations on double precision FP elements in vector mode (four at a time).\n",
        },
        {
          title = "Matching between your loop (in the source code) and the binary loop",
          txt = "The binary loop is composed of 50 FP arithmetical operations:\n - 50: addition or subtraction\nThe binary loop is loading 920 bytes (115 double precision FP elements).\nThe binary loop is storing 404 bytes (50 double precision FP elements).",
        },
        {
          title = "Arithmetic intensity",
          txt = "Arithmetic intensity is 0.04 FP operations per loaded or stored byte.",
        },
      },
      expert = {
        {
          title = "General properties",
          txt = "nb instructions    : 219\nnb uops            : 235\nloop length        : 1087\nused x86 registers : 15\nused mmx registers : 0\nused xmm registers : 16\nused ymm registers : 16\nused zmm registers : 0\nnb stack references: 18\n",
        },
        {
          title = "Front-end",
          txt = "MACRO FUSION NOT POSSIBLE\nFIT IN UOP CACHE\nmicro-operation queue: 58.75 cycles\nfront end            : 58.75 cycles\n",
        },
        {
          title = "Back-end",
          txt = "       | P0    | P1    | P2    | P3    | P4    | P5\n------------------------------------------------------\nuops   | 43.33 | 43.33 | 70.00 | 70.00 | 41.00 | 43.33\ncycles | 43.33 | 43.33 | 70.00 | 70.00 | 41.00 | 43.33\n\nCycles executing div or sqrt instructions: NA\n",
        },
        {
          title = "Cycles summary",
          txt = "Front-end : 58.75\nDispatch  : 70.00\nOverall L1: 70.00\n",
        },
        {
          title = "Vectorization ratios",
          txt = "INT\nall    : 0%\nload   : 0%\nstore  : NA (no store vectorizable/vectorized instructions)\nmul    : NA (no mul vectorizable/vectorized instructions)\nadd-sub: 0%\nfma    : NA (no fma vectorizable/vectorized instructions)\nother  : 0%\nFP\nall     : 28%\nload    : 25%\nstore   : 25%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : 14%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 100%\nINT+FP\nall     : 24%\nload    : 22%\nstore   : 25%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : 13%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 43%\n",
        },
        {
          title = "Vector efficiency ratios",
          txt = "INT\nall    : 35%\nload   : 28%\nstore  : NA (no store vectorizable/vectorized instructions)\nmul    : NA (no mul vectorizable/vectorized instructions)\nadd-sub: 25%\nfma    : NA (no fma vectorizable/vectorized instructions)\nother  : 36%\nFP\nall     : 34%\nload    : 31%\nstore   : 31%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : 35%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 50%\nINT+FP\nall     : 34%\nload    : 31%\nstore   : 31%\nmul     : NA (no mul vectorizable/vectorized instructions)\nadd-sub : 35%\nfma     : NA (no fma vectorizable/vectorized instructions)\ndiv/sqrt: NA (no div/sqrt vectorizable/vectorized instructions)\nother   : 42%\n",
        },
        {
          title = "Cycles and memory resources usage",
          txt = "Assuming all data fit into the L1 cache, each iteration of the binary loop takes 70.00 cycles. At this rate:\n - 41% of peak load performance is reached (13.14 out of 32.00 bytes loaded per cycle (GB/s @ 1GHz))\n - 36% of peak store performance is reached (5.77 out of 16.00 bytes stored per cycle (GB/s @ 1GHz))\n",
        },
        {
          title = "Front-end bottlenecks",
          txt = "Found no such bottlenecks.",
        },
        {
          title = "ASM code",
          txt = "In the binary file, the address of the loop is: 1e020\n\nInstruction                               | Nb FU | P0   | P1   | P2   | P3   | P4 | P5   | Latency | Recip. throughput\n-----------------------------------------------------------------------------------------------------------------------\nMOV 0x70(%RSP),%RDI                       | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 2       | 0.50\nMOVSXD %R9D,%R8                           | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nMOV 0x68(%RSP),%RDX                       | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 2       | 0.50\nMOV 0x58(%RSP),%RAX                       | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 2       | 0.50\nIMUL %R14,%RDI                            | 1     | 0    | 1    | 0    | 0    | 0  | 0    | 3       | 1\nIMUL %R8,%RDX                             | 1     | 0    | 1    | 0    | 0    | 0  | 0    | 3       | 1\nADD 0x78(%RSP),%RDI                       | 1     | 0.33 | 0.33 | 0.50 | 0.50 | 0  | 0.33 | 1       | 0.50\nIMUL %R14,%RAX                            | 1     | 0    | 1    | 0    | 0    | 0  | 0    | 3       | 1\nIMUL 0x50(%RSP),%R8                       | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nADD 0x60(%RSP),%RAX                       | 1     | 0.33 | 0.33 | 0.50 | 0.50 | 0  | 0.33 | 1       | 0.50\nADD %RDX,%RDI                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nLEA 0x6(%RDI),%RBX                        | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nADD %RAX,%R8                              | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nLEA 0x30(,%R8,8),%RCX                     | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nLEA 0xd0(,%RDI,8),%RAX                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nLEA (%R11,%RCX,1),%RSI                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nLEA -0xa0(%R10,%RAX,1),%RDX               | 1     | 0    | 1    | 0    | 0    | 0  | 0    | 3       | 1\nLEA 0xa0(%R11,%RCX,1),%RCX                | 1     | 0    | 1    | 0    | 0    | 0  | 0    | 3       | 1\nCMP %RCX,%RDX                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nSETAE %CL                                 | 1     | 0.50 | 0    | 0    | 0    | 0  | 0.50 | 1       | 0.50\nADD %R10,%RAX                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nCMP %RAX,%RSI                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nSETAE %AL                                 | 1     | 0.50 | 0    | 0    | 0    | 0  | 0.50 | 1       | 0.50\nOR %AL,%CL                                | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nJE 1e52f <add_._omp_fn.0+0x66f>           | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nCMPL $0x2,0x44(%RSP)                      | 1     | 0.33 | 0.33 | 0.50 | 0.50 | 0  | 0.33 | 1       | 0.50\nJBE 1e52f <add_._omp_fn.0+0x66f>          | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nLEA (%RDX,%R15,1),%RBX                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nMOV %RDX,%RAX                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nMOV %RSI,%RCX                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nTEST $0x20,%R15B                          | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nJE 1e1a8 <add_._omp_fn.0+0x2e8>           | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nVMOVUPD 0x20(%RSI),%XMM2                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVINSERTF128 $0x1,0x30(%RSI),%YMM2,%YMM0   | 2     | 0    | 0    | 0.50 | 0.50 | 0  | 1    | 2       | 1\nADD $0xa0,%RAX                            | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nADD $0xa0,%RCX                            | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nVMOVUPD 0x40(%RSI),%XMM7                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVINSERTF128 $0x1,0x50(%RSI),%YMM7,%YMM8   | 2     | 0    | 0    | 0.50 | 0.50 | 0  | 1    | 2       | 1\nVMOVUPD 0x60(%RSI),%XMM12                 | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVINSERTF128 $0x1,0x70(%RSI),%YMM12,%YMM13 | 2     | 0    | 0    | 0.50 | 0.50 | 0  | 1    | 2       | 1\nVMOVUPD 0x20(%RDX),%XMM1                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVINSERTF128 $0x1,0x30(%RDX),%YMM1,%YMM3   | 2     | 0    | 0    | 0.50 | 0.50 | 0  | 1    | 2       | 1\nVADDPD %YMM0,%YMM3,%YMM6                  | 1     | 0    | 1    | 0    | 0    | 0  | 0    | 3       | 1\nVMOVUPD 0x40(%RDX),%XMM4                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVINSERTF128 $0x1,0x50(%RDX),%YMM4,%YMM5   | 2     | 0    | 0    | 0.50 | 0.50 | 0  | 1    | 2       | 1\nVADDPD %YMM8,%YMM5,%YMM9                  | 1     | 0    | 1    | 0    | 0    | 0  | 0    | 3       | 1\nVMOVUPD 0x80(%RSI),%XMM3                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVINSERTF128 $0x1,0x90(%RSI),%YMM3,%YMM2   | 2     | 0    | 0    | 0.50 | 0.50 | 0  | 1    | 2       | 1\nVMOVUPD (%RSI),%XMM5                      | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVINSERTF128 $0x1,0x10(%RSI),%YMM5,%YMM7   | 2     | 0    | 0    | 0.50 | 0.50 | 0  | 1    | 2       | 1\nVMOVUPS %XMM6,0x20(%RDX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVEXTRACTF128 $0x1,%YMM6,0x30(%RDX)        | 2     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 2       | 1\nVMOVUPD (%RDX),%XMM4                      | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVINSERTF128 $0x1,0x10(%RDX),%YMM4,%YMM8   | 2     | 0    | 0    | 0.50 | 0.50 | 0  | 1    | 2       | 1\nVMOVUPS %XMM9,0x40(%RDX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVEXTRACTF128 $0x1,%YMM9,0x50(%RDX)        | 2     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 2       | 1\nVMOVUPD 0x60(%RDX),%XMM10                 | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVINSERTF128 $0x1,0x70(%RDX),%YMM10,%YMM11 | 2     | 0    | 0    | 0.50 | 0.50 | 0  | 1    | 2       | 1\nVADDPD %YMM7,%YMM8,%YMM10                 | 1     | 0    | 1    | 0    | 0    | 0  | 0    | 3       | 1\nVADDPD %YMM13,%YMM11,%YMM14               | 1     | 0    | 1    | 0    | 0    | 0  | 0    | 3       | 1\nVMOVUPD 0x80(%RDX),%XMM15                 | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVINSERTF128 $0x1,0x90(%RDX),%YMM15,%YMM1  | 2     | 0    | 0    | 0.50 | 0.50 | 0  | 1    | 2       | 1\nVADDPD %YMM2,%YMM1,%YMM0                  | 1     | 0    | 1    | 0    | 0    | 0  | 0    | 3       | 1\nVMOVUPS %XMM10,(%RDX)                     | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVEXTRACTF128 $0x1,%YMM10,0x10(%RDX)       | 2     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 2       | 1\nVMOVUPS %XMM14,0x60(%RDX)                 | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVEXTRACTF128 $0x1,%YMM14,0x70(%RDX)       | 2     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 2       | 1\nVMOVUPS %XMM0,0x80(%RDX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVEXTRACTF128 $0x1,%YMM0,0x90(%RDX)        | 2     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 2       | 1\nCMP %RBX,%RAX                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nJE 1e3be <add_._omp_fn.0+0x4fe>           | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nNOPL (%RAX)                               | 1     | 0    | 0    | 0    | 0    | 0  | 0    | 0       | 0.25\nCMP 0x40(%RSP),%R13D                      | 1     | 0.33 | 0.33 | 0.50 | 0.50 | 0  | 0.33 | 1       | 0.50\nJE 1e4fe <add_._omp_fn.0+0x63e>           | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nMOV 0x30(%RSP),%RDX                       | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 2       | 0.50\nLEA (%RDI,%RDX,1),%RAX                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nLEA (%RDX,%R8,1),%RBX                     | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nLEA (%R10,%RAX,8),%RCX                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nLEA (%R11,%RBX,8),%RSI                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nVMOVSD 0x8(%RCX),%XMM10                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVADDSD 0x8(%RSI),%XMM10,%XMM11            | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD 0x10(%RCX),%XMM14                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD 0x18(%RCX),%XMM1                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD %XMM11,0x8(%RCX)                   | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x10(%RSI),%XMM14,%XMM15           | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD 0x20(%RCX),%XMM0                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD 0x28(%RCX),%XMM2                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD %XMM15,0x10(%RCX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x18(%RSI),%XMM1,%XMM3             | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM3,0x18(%RCX)                   | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x20(%RSI),%XMM0,%XMM13            | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM13,0x20(%RCX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x28(%RSI),%XMM2,%XMM4             | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM4,0x28(%RCX)                   | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nCMP 0x3c(%RSP),%R13D                      | 1     | 0.33 | 0.33 | 0.50 | 0.50 | 0  | 0.33 | 1       | 0.50\nJL 1e4fe <add_._omp_fn.0+0x63e>           | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nMOV 0x28(%RSP),%RDX                       | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 2       | 0.50\nLEA (%RDI,%RDX,1),%RAX                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nLEA (%R8,%RDX,1),%RBX                     | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nLEA (%R10,%RAX,8),%RCX                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nLEA (%R11,%RBX,8),%RSI                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nVMOVSD 0x8(%RCX),%XMM8                    | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVADDSD 0x8(%RSI),%XMM8,%XMM5              | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD 0x10(%RCX),%XMM7                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD 0x18(%RCX),%XMM6                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD %XMM5,0x8(%RCX)                    | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x10(%RSI),%XMM7,%XMM9             | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD 0x20(%RCX),%XMM10                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD 0x28(%RCX),%XMM14                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD %XMM9,0x10(%RCX)                   | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x18(%RSI),%XMM6,%XMM12            | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM12,0x18(%RCX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x20(%RSI),%XMM10,%XMM11           | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM11,0x20(%RCX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x28(%RSI),%XMM14,%XMM15           | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM15,0x28(%RCX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nCMP 0x38(%RSP),%R13D                      | 1     | 0.33 | 0.33 | 0.50 | 0.50 | 0  | 0.33 | 1       | 0.50\nJL 1e4fe <add_._omp_fn.0+0x63e>           | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nMOV 0x20(%RSP),%RDX                       | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 2       | 0.50\nADD %RDX,%RDI                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nADD %RDX,%R8                              | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nLEA (%R10,%RDI,8),%RDI                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nLEA (%R11,%R8,8),%R8                      | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nVMOVSD 0x8(%RDI),%XMM1                    | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVADDSD 0x8(%R8),%XMM1,%XMM3               | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD 0x10(%RDI),%XMM0                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD 0x18(%RDI),%XMM2                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD %XMM3,0x8(%RDI)                    | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x10(%R8),%XMM0,%XMM13             | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD 0x20(%RDI),%XMM8                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD 0x28(%RDI),%XMM7                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD %XMM13,0x10(%RDI)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x18(%R8),%XMM2,%XMM4              | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM4,0x18(%RDI)                   | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x20(%R8),%XMM8,%XMM5              | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM5,0x20(%RDI)                   | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x28(%R8),%XMM7,%XMM9              | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM9,0x28(%RDI)                   | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nCMP 0x4c(%RSP),%R12D                      | 1     | 0.33 | 0.33 | 0.50 | 0.50 | 0  | 0.33 | 1       | 0.50\nJE 1e51d <add_._omp_fn.0+0x65d>           | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nADD $0x1,%R9D                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nCMP %R9D,0x48(%RSP)                       | 1     | 0.33 | 0.33 | 0.50 | 0.50 | 0  | 0.33 | 1       | 0.50\nJLE 1e7d4 <add_._omp_fn.0+0x914>          | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nADD $0x1,%R12D                            | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nJMP 1e020 <add_._omp_fn.0+0x160>          | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 2\nADD 0x10(%RSP),%RDI                       | 1     | 0.33 | 0.33 | 0.50 | 0.50 | 0  | 0.33 | 1       | 0.50\nLEA (%R10,%RBX,8),%RAX                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nMOV %RSI,%RDX                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nMOV 0x8(%RSP),%RCX                        | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 2       | 0.50\nLEA (%RCX,%RDI,8),%RBX                    | 1     | 0.50 | 0.50 | 0    | 0    | 0  | 0    | 1       | 0.50\nMOV %RBX,%RDI                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nSUB %RAX,%RDI                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nSUB $0x28,%RDI                            | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nSHR $0x3,%RDI                             | 1     | 0.50 | 0    | 0    | 0    | 0  | 0.50 | 1       | 0.50\nADD $0x1,%RDI                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nAND $0x3,%EDI                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nJE 1e66d <add_._omp_fn.0+0x7ad>           | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nCMP $0x1,%RDI                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nJE 1e612 <add_._omp_fn.0+0x752>           | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nCMP $0x2,%RDI                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nJE 1e5c0 <add_._omp_fn.0+0x700>           | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nVMOVSD (%RAX),%XMM6                       | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nADD $0x28,%RDX                            | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nADD $0x28,%RAX                            | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nVADDSD (%RSI),%XMM6,%XMM12                | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD -0x20(%RAX),%XMM10                 | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD -0x18(%RAX),%XMM14                 | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD %XMM12,-0x28(%RAX)                 | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x8(%RSI),%XMM10,%XMM11            | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD -0x10(%RAX),%XMM1                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD -0x8(%RAX),%XMM0                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD %XMM11,-0x20(%RAX)                 | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x10(%RSI),%XMM14,%XMM15           | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM15,-0x18(%RAX)                 | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x18(%RSI),%XMM1,%XMM3             | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM3,-0x10(%RAX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD 0x20(%RSI),%XMM0,%XMM13            | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM13,-0x8(%RAX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVMOVSD (%RAX),%XMM2                       | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nADD $0x28,%RDX                            | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nADD $0x28,%RAX                            | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nVADDSD -0x28(%RDX),%XMM2,%XMM4            | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD -0x20(%RAX),%XMM8                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD -0x18(%RAX),%XMM7                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD %XMM4,-0x28(%RAX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD -0x20(%RDX),%XMM8,%XMM5            | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD -0x10(%RAX),%XMM6                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD -0x8(%RAX),%XMM10                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD %XMM5,-0x20(%RAX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD -0x18(%RDX),%XMM7,%XMM9            | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM9,-0x18(%RAX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD -0x10(%RDX),%XMM6,%XMM12           | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM12,-0x10(%RAX)                 | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD -0x8(%RDX),%XMM10,%XMM11           | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM11,-0x8(%RAX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVMOVSD (%RAX),%XMM14                      | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nADD $0x28,%RAX                            | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nADD $0x28,%RDX                            | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nVADDSD -0x28(%RDX),%XMM14,%XMM15          | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD -0x20(%RAX),%XMM1                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD -0x18(%RAX),%XMM0                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD %XMM15,-0x28(%RAX)                 | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD -0x20(%RDX),%XMM1,%XMM3            | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD -0x10(%RAX),%XMM2                  | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD -0x8(%RAX),%XMM8                   | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 3       | 0.50\nVMOVSD %XMM3,-0x20(%RAX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD -0x18(%RDX),%XMM0,%XMM13           | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM13,-0x18(%RAX)                 | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD -0x10(%RDX),%XMM2,%XMM4            | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM4,-0x10(%RAX)                  | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nVADDSD -0x8(%RDX),%XMM8,%XMM5             | 1     | 0    | 1    | 0.50 | 0.50 | 0  | 0    | 3       | 1\nVMOVSD %XMM5,-0x8(%RAX)                   | 1     | 0    | 0    | 0.50 | 0.50 | 1  | 0    | 3       | 1\nCMP %RBX,%RAX                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nJE 1e4fe <add_._omp_fn.0+0x63e>           | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 1-2\nJMP 1e4fe <add_._omp_fn.0+0x63e>          | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 2\nADDL $0x1,0x1c(%RSP)                      | 2     | 0.33 | 0.33 | 1    | 1    | 1  | 0.33 | 6       | 1\nMOV $0x1,%R9D                             | 1     | 0.33 | 0.33 | 0    | 0    | 0  | 0.33 | 1       | 0.33\nMOVSXD 0x1c(%RSP),%R14                    | 1     | 0    | 0    | 0.50 | 0.50 | 0  | 0    | 1       | 0.50\nJMP 1e514 <add_._omp_fn.0+0x654>          | 1     | 0    | 0    | 0    | 0    | 0  | 1    | 0       | 2\n",
        },
      },
      header = {
        "8% of peak computational performance is used (0.71 out of 8.00 FLOP per cycle (GFLOPS @ 1GHz))",
      },
      brief = {
      },
      gain = {
        {
          workaround = " - Try to reorganize arrays of structures to structures of arrays\n - Consider to permute loops (see vectorization gain report)\n - To reference allocatable arrays, use \"allocatable\" instead of \"pointer\" pointers or qualify them with the \"contiguous\" attribute (Fortran 2008)\n - For structures, limit to one indirection. For example, use a_b%c instead of a%b%c with a_b set to a%b before this loop\n",
          title = "Code clean check",
          txt = "Detected a slowdown caused by scalar integer instructions (typically used for address computation).\nBy removing them, you can lower the cost of an iteration from 70.00 to 61.50 cycles (1.14x speedup).",
        },
        {
          workaround = " - Try another compiler or update/tune your current one:\n  * recompile with fassociative-math (included in Ofast or ffast-math) to extend loop vectorization to FP reductions.\n - Remove inter-iterations dependences from your loop and make it unit-stride:\n  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:\nFortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)\n  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):\ndo i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)\n",
          details = "24% of SSE/AVX instructions are used in vector version (process two or more data elements in vector registers):\n - 22% of SSE/AVX loads are used in vector version.\n - 25% of SSE/AVX stores are used in vector version.\n - 13% of SSE/AVX addition or subtraction instructions are used in vector version.\n - 43% of SSE/AVX instructions that are not load, store, addition, subtraction nor multiply instructions are used in vector version.\nSince your execution units are vector units, only a fully vectorized loop can use their full power.\n",
          title = "Vectorization",
          txt = "Your loop is poorly vectorized.\nOnly 34% of vector register length is used (average across all SSE/AVX instructions).\nBy fully vectorizing your loop, you can lower the cost of an iteration from 70.00 to 34.07 cycles (2.05x speedup).",
        },
        {
          workaround = " - Read less array elements\nAll SSE and/or AVX registers are used:\nin that case, try to relax register pressure by reducing the unroll factor or splitting your loop\n - Write less array elements\n - Provide more information to your compiler:\n  * hardcode the bounds of the corresponding 'for' loop\n",
          title = "Execution units bottlenecks",
          txt = "Performance is limited by:\n - reading data from caches/RAM (load units are a bottleneck)\n - writing data to caches/RAM (the store unit is a bottleneck)\n\nBy removing all these bottlenecks, you can lower the cost of an iteration from 70.00 to 58.75 cycles (1.19x speedup).\n",
        },
      },
      potential = {
      },
    },
  common = {
    header = {
      "The loop is defined in /users/user2210/NPB3.4-MZ-MPI/BT-MZ/add.f90:24-29.\n",
      "Warnings:\n - Non-innermost loop: analyzing only self part (ignoring child loops).\n - Ignoring paths for analysis\n - Too many paths. If you really need to analyze all of the 52 paths individually, rerun with max-paths=52\n - RecMII not computed since number of paths is unknown or > max_paths\n - Streams not analyzed since number of paths is unknown or > max_paths\n",
      "Try to simplify control and/or increase the maximum number of paths per function/loop through the 'max-paths-nb' option.\n",
      "This loop has 52 execution paths.\n",
      "The presence of multiple execution paths is typically the main/first bottleneck.\nTry to simplify control inside loop: ideally, try to remove all conditional expressions, for example by (if applicable):\n - hoisting them (moving them outside the loop)\n - turning them into conditional moves, MIN or MAX\n\n",
      "Ex: if (x<0) x=0 => x = max(0,x) (Fortran instrinsic procedure)\n",
    },
    nb_paths = 52,
  },
}
