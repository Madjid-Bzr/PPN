_cqa_text_report = {
  paths = {
    {
      hint = {
        {
          workaround = " - Try to reorganize arrays of structures to structures of arrays\n - Consider to permute loops (see vectorization gain report)\n",
          details = " - Constant unknown stride: 8 occurrence(s)\nNon-unit stride (uncontiguous) accesses are not efficiently using data caches\n",
          title = "Slow data structures access",
          txt = "Detected data structures (typically arrays) that cannot be efficiently read/written",
        },
        {
          workaround = "Use vector aligned instructions:\n 1) The GNU Fortran compiler does not support allocation alignment and does not feature directives to benefit from aligned data.\n 2) Use another compiler or locally use C code (for instance via libraries)\n",
          details = " - VEXTRACTF128: 1 occurrences\n",
          title = "Vector unaligned load/store instructions",
          txt = "Detected 1 suboptimal vector unaligned load/store instructions.\n",
        },
        {
          title = "Type of elements and instruction set",
          txt = "80 SSE or AVX instructions are processing arithmetic or math operations on double precision FP elements in scalar mode (one at a time).\n11 AVX instructions are processing arithmetic or math operations on double precision FP elements in vector mode (four at a time).\n",
        },
        {
          title = "Matching between your loop (in the source code) and the binary loop",
          txt = "The binary loop is composed of 224 FP arithmetical operations:\n - 100: addition or subtraction (all inside FMA instructions)\n - 115: multiply (100 inside FMA instructions)\n - 9: divide\nThe binary loop is loading 720 bytes (90 double precision FP elements).\nThe binary loop is storing 72 bytes (9 double precision FP elements).",
        },
        {
          title = "Arithmetic intensity",
          txt = "Arithmetic intensity is 0.28 FP operations per loaded or stored byte.",
        },
      },
      expert = {
        {
          title = "General properties",
          txt = "nb instructions    : 158\nloop length        : 977\nused x86 registers : 10\nused mmx registers : 0\nused xmm registers : 31\nused ymm registers : 11\nused zmm registers : 21\nnb stack references: 4\n",
        },
        {
          title = "Front-end",
          txt = "MACRO FUSION NOT POSSIBLE\ninstruction fetch    : 62.00 cycles\ninstruction queue    : 79.00 cycles\ndecoding             : 79.00 cycles\nmicro-operation queue: 79.00 cycles\nfront end            : 79.00 cycles\n",
        },
        {
          title = "Back-end",
          txt = "       | P0    | P1    | P2    | P3    | P4   | P5   | P6\n-----------------------------------------------------------\nuops   | 63.50 | 63.50 | 33.00 | 33.00 | 3.50 | 3.50 | 9.00\ncycles | 63.50 | 63.50 | 33.00 | 33.00 | 3.50 | 3.50 | 9.00\n\nCycles executing div or sqrt instructions: 108.00-288.00\nLongest recurrence chain latency (RecMII): 1.00\n",
        },
        {
          title = "Cycles summary",
          txt = "Front-end : 79.00\nDispatch  : 63.50\nDIV/SQRT  : 108.00-288.00\nData deps.: 1.00\nOverall L1: 108.00-288.00\n",
        },
        {
          title = "Vectorization ratios",
          txt = "all     : 17%\nload    : 19%\nstore   : 0%\nmul     : 8%\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : 14%\ndiv/sqrt: 0%\nother   : 41%\n",
        },
        {
          title = "Vector efficiency ratios",
          txt = "all     : 23%\nload    : 19%\nstore   : 12%\nmul     : 15%\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : 17%\ndiv/sqrt: 12%\nother   : 46%\n",
        },
        {
          title = "Cycles and memory resources usage",
          txt = "Assuming all data fit into the L1 cache, each iteration of the binary loop takes 288.00 cycles. At this rate:\n - 1% of peak load performance is reached (2.50 out of 128.00 bytes loaded per cycle (GB/s @ 1GHz))\n - 0% of peak store performance is reached (0.25 out of 64.00 bytes stored per cycle (GB/s @ 1GHz))\n",
        },
        {
          title = "Front-end bottlenecks",
          txt = "Found no such bottlenecks.",
        },
        {
          title = "ASM code",
          txt = "In the binary file, the address of the loop is: 25fb0\n\nInstruction                                 | Nb FU | P0   | P1   | P2   | P3   | P4   | P5   | P6 | Latency | Recip. throughput\n--------------------------------------------------------------------------------------------------------------------------------\nVMOVSD -0x20(%RDX),%XMM8                    | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nLEA 0x28(%R9),%R9                           | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nVBROADCASTSD %XMM8,%YMM0                    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 5       | 1\nLEA 0xc8(%RAX),%RAX                         | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nVMULSD 0x50(%RCX,%RDI,1),%XMM8,%XMM8        | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVMOVSD -0x28(%R9),%XMM22                    | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVMULPD (%RCX,%RSI,1),%YMM0,%YMM0            | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVBROADCASTSD %XMM22,%ZMM13                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 5       | 1\nVMOVSD -0x20(%R9),%XMM19                    | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVMOVSD -0x18(%R9),%XMM17                    | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVBROADCASTSD %XMM19,%ZMM12                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 5       | 1\nVMOVSD -0x10(%R9),%XMM15                    | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVBROADCASTSD %XMM17,%ZMM10                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 5       | 1\nVMOVSD -0x8(%R9),%XMM2                      | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVBROADCASTSD %XMM15,%YMM5                   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 5       | 1\nVMOVSD -0x18(%RDX),%XMM18                   | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVBROADCASTSD %XMM2,%YMM3                    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 5       | 1\nVMOVSD -0x10(%RDX),%XMM16                   | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVBROADCASTSD %XMM18,%ZMM11                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 5       | 1\nVMOVSD -0x8(%RDX),%XMM14                    | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVBROADCASTSD %XMM16,%ZMM7                   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 5       | 1\nVMOVSD (%RDX),%XMM1                         | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVBROADCASTSD %XMM14,%YMM4                   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 5       | 1\nVBROADCASTSD %XMM1,%YMM9                    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 5       | 1\nVFMADD231PD (%RBX,%RSI,1),%YMM13,%YMM0      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132SD 0x50(%RBX,%RDI,1),%XMM8,%XMM22  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD231PD 0x28(%RBX,%RSI,1),%YMM12,%YMM0  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132SD 0x78(%RBX,%RDI,1),%XMM22,%XMM19 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD231PD 0x28(%RCX,%RSI,1),%YMM11,%YMM0  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132SD 0x78(%RCX,%RDI,1),%XMM19,%XMM18 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD231PD 0x50(%RBX,%RSI,1),%YMM10,%YMM0  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132SD 0xa0(%RBX,%RDI,1),%XMM18,%XMM17 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD231PD 0x50(%RCX,%RSI,1),%YMM7,%YMM0   | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132SD 0xa0(%RCX,%RDI,1),%XMM17,%XMM16 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD231PD 0x78(%RBX,%RSI,1),%YMM5,%YMM0   | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132SD 0xc8(%RBX,%RDI,1),%XMM16,%XMM15 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD231PD 0x78(%RCX,%RSI,1),%YMM4,%YMM0   | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132SD 0xc8(%RCX,%RDI,1),%XMM15,%XMM14 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132PD 0xa0(%RBX,%RSI,1),%YMM0,%YMM3   | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132SD 0xf0(%RBX,%RDI,1),%XMM14,%XMM2  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132PD 0xa0(%RCX,%RSI,1),%YMM3,%YMM9   | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132SD 0xf0(%RCX,%RDI,1),%XMM2,%XMM1   | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213PD 0x28(%RDX,%R10,8),%YMM6,%YMM9  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213SD 0x28(%RDX),%XMM21,%XMM1        | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVMOVSD -0xc8(%RAX),%XMM22                   | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVDIVSD %XMM22,%XMM20,%XMM15                 | 3     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 40-42   | 12-32\nVMOVAPD %ZMM9,%ZMM25                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVSD -0xa0(%RAX),%XMM26                   | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVMOVAPD %ZMM1,%ZMM29                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVSD -0x78(%RAX),%XMM24                   | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nCMP %R9,%R11                                | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nVUNPCKHPD %XMM9,%XMM9,%XMM1                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nLEA 0x28(%RDX),%RDX                         | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nVEXTRACTF128 $0x1,%YMM9,%XMM12              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nLEA 0xc8(%RDI),%RDI                         | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nVMOVSD -0x50(%RAX),%XMM23                   | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nLEA 0xc8(%RSI),%RSI                         | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nVMOVSD -0x28(%RAX),%XMM19                   | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVMULSD -0xc0(%RAX),%XMM15,%XMM10            | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVMULSD -0xb8(%RAX),%XMM15,%XMM16            | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVMULSD -0xb0(%RAX),%XMM15,%XMM13            | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVMOVAPD %ZMM10,%ZMM27                       | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVAPD %ZMM10,%ZMM18                       | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVFNMADD213SD -0x98(%RAX),%XMM26,%XMM27      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVDIVSD %XMM27,%XMM20,%XMM0                  | 3     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 40-42   | 12-32\nVMOVAPD %ZMM16,%ZMM28                       | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVAPD %ZMM16,%ZMM4                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVFNMADD213SD -0x90(%RAX),%XMM26,%XMM28      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213SD -0x70(%RAX),%XMM24,%XMM18      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVMOVSD %XMM13,%XMM13,%XMM14                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVFNMADD213SD -0x68(%RAX),%XMM24,%XMM4       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213SD -0x88(%RAX),%XMM26,%XMM14      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVMOVAPD %ZMM10,%ZMM17                       | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVSD %XMM10,%XMM10,%XMM7                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVMOVAPD %ZMM16,%ZMM30                       | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVFNMADD132SD %XMM9,%XMM1,%XMM10             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVMOVAPD %ZMM16,%ZMM5                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVFNMADD132SD %XMM9,%XMM12,%XMM16            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVUNPCKHPD %XMM12,%XMM12,%XMM9               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVMOVSD %XMM13,%XMM13,%XMM11                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVMOVSD %XMM13,%XMM13,%XMM3                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVMOVSD %XMM13,%XMM13,%XMM2                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVFNMADD213SD -0x20(%RAX),%XMM19,%XMM7       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213SD -0x48(%RAX),%XMM23,%XMM17      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM13,%XMM25,%XMM9            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVMULSD -0xa8(%RAX),%XMM15,%XMM8             | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213SD -0x60(%RAX),%XMM24,%XMM11      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213SD -0x38(%RAX),%XMM23,%XMM3       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213SD -0x10(%RAX),%XMM19,%XMM2       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVMULSD %XMM0,%XMM28,%XMM28                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213SD -0x40(%RAX),%XMM23,%XMM30      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213SD -0x18(%RAX),%XMM19,%XMM5       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVMOVSD %XMM8,%XMM8,%XMM15                   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVMOVSD %XMM8,%XMM8,%XMM12                   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVMOVAPD %ZMM8,%ZMM31                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVSD %XMM8,%XMM8,%XMM1                    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVFNMADD231SD %XMM28,%XMM18,%XMM4            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD132SD %XMM25,%XMM29,%XMM8            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVMOVAPD %ZMM4,%ZMM29                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVFNMADD213SD -0x80(%RAX),%XMM26,%XMM15      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVMULSD %XMM0,%XMM14,%XMM4                   | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM28,%XMM17,%XMM30           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213SD -0x58(%RAX),%XMM24,%XMM12      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213SD -0x30(%RAX),%XMM23,%XMM31      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVMULSD %XMM0,%XMM15,%XMM13                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213SD -0x8(%RAX),%XMM19,%XMM1        | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM28,%XMM7,%XMM5             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM28,%XMM10,%XMM16           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM4,%XMM7,%XMM2              | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM4,%XMM18,%XMM11            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM4,%XMM17,%XMM3             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD132SD %XMM10,%XMM9,%XMM4             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVDIVSD %XMM29,%XMM20,%XMM9                  | 3     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 40-42   | 12-32\nVFNMADD231SD %XMM13,%XMM7,%XMM1             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM13,%XMM18,%XMM12           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM13,%XMM17,%XMM31           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD132SD %XMM10,%XMM8,%XMM13            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVMOVAPD %ZMM31,%ZMM8                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVSD %XMM1,%XMM1,%XMM14                   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVMULSD %XMM9,%XMM11,%XMM11                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVMULSD %XMM9,%XMM12,%XMM15                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM11,%XMM30,%XMM3            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVDIVSD %XMM3,%XMM20,%XMM31                  | 3     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 40-42   | 12-32\nVFNMADD231SD %XMM11,%XMM5,%XMM2             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD132SD %XMM16,%XMM4,%XMM11            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM15,%XMM30,%XMM8            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM15,%XMM5,%XMM14            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD132SD %XMM16,%XMM13,%XMM15           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVMULSD %XMM8,%XMM31,%XMM28                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM28,%XMM2,%XMM14            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM28,%XMM11,%XMM15           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVDIVSD %XMM14,%XMM15,%XMM12                 | 3     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 40-42   | 12-32\nVFNMADD231SD %XMM12,%XMM2,%XMM11            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVDIVSD %XMM3,%XMM11,%XMM1                   | 3     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 40-42   | 12-32\nVMOVAPD %ZMM30,%ZMM3                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVAPD %ZMM18,%ZMM2                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVSD %XMM12,(%RDX)                        | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVMOVSD %XMM11,0x98(%RSP)                    | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVFNMADD132SD %XMM1,%XMM16,%XMM3             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM12,%XMM5,%XMM3             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVDIVSD %XMM29,%XMM3,%XMM5                   | 3     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 40-42   | 12-32\nVMOVSD %XMM1,-0x8(%RDX)                     | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVMOVSD %XMM3,0x90(%RSP)                     | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVFNMADD132SD %XMM5,%XMM10,%XMM2             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM1,%XMM17,%XMM2             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM12,%XMM7,%XMM2             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVDIVSD %XMM27,%XMM2,%XMM10                  | 3     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 40-42   | 12-32\nVMOVSD %XMM5,-0x10(%RDX)                    | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVMOVSD %XMM2,0x88(%RSP)                     | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVMOVSD %XMM10,-0x18(%RDX)                   | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVFNMADD132SD %XMM26,%XMM25,%XMM10           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD132SD %XMM24,%XMM10,%XMM5            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD132SD %XMM23,%XMM5,%XMM1             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD132SD %XMM19,%XMM1,%XMM12            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVDIVSD %XMM22,%XMM12,%XMM7                  | 3     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 40-42   | 12-32\nVMOVSD %XMM12,0x80(%RSP)                    | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVMOVSD %XMM7,-0x20(%RDX)                    | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nJNE 25fb0 <blts_+0x500>                     | 1     | 0    | 0    | 0    | 0    | 0    | 1    | 0  | 0       | 1-2\n",
        },
      },
      header = {
        "2% of peak computational performance is used (0.78 out of 32.00 FLOP per cycle (GFLOPS @ 1GHz))",
      },
      brief = {
      },
      gain = {
        {
          workaround = " - Try another compiler or update/tune your current one:\n  * recompile with fassociative-math (included in Ofast or ffast-math) to extend loop vectorization to FP reductions.\n - Remove inter-iterations dependences from your loop and make it unit-stride:\n  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:\nFortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)\n  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):\ndo i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)\n",
          details = "17% of SSE/AVX instructions are used in vector version (process two or more data elements in vector registers):\n - 19% of SSE/AVX loads are used in vector version.\n - 0% of SSE/AVX stores are used in vector version.\n - 8% of SSE/AVX multiply instructions are used in vector version.\n - 14% of SSE/AVX fused multiply-add instructions are used in vector version.\n - 0% of SSE/AVX nil are used in vector version.\n - 41% of SSE/AVX instructions that are not load, store, addition, subtraction nor multiply instructions are used in vector version.\nSince your execution units are vector units, only a fully vectorized loop can use their full power.\n",
          title = "Vectorization",
          txt = "Your loop is not vectorized.\nOnly 23% of vector register length is used (average across all SSE/AVX instructions).\nBy fully vectorizing your loop, you can lower the cost of an iteration from 288.00 to 36.00 cycles (8.00x speedup).",
        },
        {
          workaround = " - Reduce the number of division or square root instructions:\n  * If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast\n - Check whether you really need double precision. If not, switch to single precision to speedup execution\n",
          title = "Execution units bottlenecks",
          txt = "Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).\n\nBy removing all these bottlenecks, you can lower the cost of an iteration from 288.00 to 79.00 cycles (3.65x speedup).\n",
        },
      },
      potential = {
        {
          title = "Expensive FP math instructions/calls",
          txt = "Detected performance impact from expensive FP math instructions/calls.\nBy removing/reexpressing them, you can lower the cost of an iteration from 288.00 to 79.00 cycles (3.65x speedup).",
        },
        {
          title = "FMA",
          txt = "Detected 100 FMA (fused multiply-add) operations.",
        },
      },
    },
  },
  AVG = {
      hint = {
        {
          workaround = " - Try to reorganize arrays of structures to structures of arrays\n - Consider to permute loops (see vectorization gain report)\n",
          details = " - Constant unknown stride: 8 occurrence(s)\nNon-unit stride (uncontiguous) accesses are not efficiently using data caches\n",
          title = "Slow data structures access",
          txt = "Detected data structures (typically arrays) that cannot be efficiently read/written",
        },
        {
          workaround = "Use vector aligned instructions:\n 1) The GNU Fortran compiler does not support allocation alignment and does not feature directives to benefit from aligned data.\n 2) Use another compiler or locally use C code (for instance via libraries)\n",
          details = " - VEXTRACTF128: 1 occurrences\n",
          title = "Vector unaligned load/store instructions",
          txt = "Detected 1 suboptimal vector unaligned load/store instructions.\n",
        },
        {
          title = "Type of elements and instruction set",
          txt = "80 SSE or AVX instructions are processing arithmetic or math operations on double precision FP elements in scalar mode (one at a time).\n11 AVX instructions are processing arithmetic or math operations on double precision FP elements in vector mode (four at a time).\n",
        },
        {
          title = "Matching between your loop (in the source code) and the binary loop",
          txt = "The binary loop is composed of 224 FP arithmetical operations:\n - 100: addition or subtraction (all inside FMA instructions)\n - 115: multiply (100 inside FMA instructions)\n - 9: divide\nThe binary loop is loading 720 bytes (90 double precision FP elements).\nThe binary loop is storing 72 bytes (9 double precision FP elements).",
        },
        {
          title = "Arithmetic intensity",
          txt = "Arithmetic intensity is 0.28 FP operations per loaded or stored byte.",
        },
      },
      expert = {
        {
          title = "General properties",
          txt = "nb instructions    : 158\nloop length        : 977\nused x86 registers : 10\nused mmx registers : 0\nused xmm registers : 31\nused ymm registers : 11\nused zmm registers : 21\nnb stack references: 4\n",
        },
        {
          title = "Front-end",
          txt = "MACRO FUSION NOT POSSIBLE\ninstruction fetch    : 62.00 cycles\ninstruction queue    : 79.00 cycles\ndecoding             : 79.00 cycles\nmicro-operation queue: 79.00 cycles\nfront end            : 79.00 cycles\n",
        },
        {
          title = "Back-end",
          txt = "       | P0    | P1    | P2    | P3    | P4   | P5   | P6\n-----------------------------------------------------------\nuops   | 63.50 | 63.50 | 33.00 | 33.00 | 3.50 | 3.50 | 9.00\ncycles | 63.50 | 63.50 | 33.00 | 33.00 | 3.50 | 3.50 | 9.00\n\nCycles executing div or sqrt instructions: 108.00-288.00\nLongest recurrence chain latency (RecMII): 1.00\n",
        },
        {
          title = "Cycles summary",
          txt = "Front-end : 79.00\nDispatch  : 63.50\nDIV/SQRT  : 108.00-288.00\nData deps.: 1.00\nOverall L1: 108.00-288.00\n",
        },
        {
          title = "Vectorization ratios",
          txt = "all     : 17%\nload    : 19%\nstore   : 0%\nmul     : 8%\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : 14%\ndiv/sqrt: 0%\nother   : 41%\n",
        },
        {
          title = "Vector efficiency ratios",
          txt = "all     : 23%\nload    : 19%\nstore   : 12%\nmul     : 15%\nadd-sub : NA (no add-sub vectorizable/vectorized instructions)\nfma     : 17%\ndiv/sqrt: 12%\nother   : 46%\n",
        },
        {
          title = "Cycles and memory resources usage",
          txt = "Assuming all data fit into the L1 cache, each iteration of the binary loop takes 288.00 cycles. At this rate:\n - 1% of peak load performance is reached (2.50 out of 128.00 bytes loaded per cycle (GB/s @ 1GHz))\n - 0% of peak store performance is reached (0.25 out of 64.00 bytes stored per cycle (GB/s @ 1GHz))\n",
        },
        {
          title = "Front-end bottlenecks",
          txt = "Found no such bottlenecks.",
        },
        {
          title = "ASM code",
          txt = "In the binary file, the address of the loop is: 25fb0\n\nInstruction                                 | Nb FU | P0   | P1   | P2   | P3   | P4   | P5   | P6 | Latency | Recip. throughput\n--------------------------------------------------------------------------------------------------------------------------------\nVMOVSD -0x20(%RDX),%XMM8                    | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nLEA 0x28(%R9),%R9                           | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nVBROADCASTSD %XMM8,%YMM0                    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 5       | 1\nLEA 0xc8(%RAX),%RAX                         | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nVMULSD 0x50(%RCX,%RDI,1),%XMM8,%XMM8        | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVMOVSD -0x28(%R9),%XMM22                    | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVMULPD (%RCX,%RSI,1),%YMM0,%YMM0            | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVBROADCASTSD %XMM22,%ZMM13                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 5       | 1\nVMOVSD -0x20(%R9),%XMM19                    | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVMOVSD -0x18(%R9),%XMM17                    | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVBROADCASTSD %XMM19,%ZMM12                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 5       | 1\nVMOVSD -0x10(%R9),%XMM15                    | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVBROADCASTSD %XMM17,%ZMM10                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 5       | 1\nVMOVSD -0x8(%R9),%XMM2                      | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVBROADCASTSD %XMM15,%YMM5                   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 5       | 1\nVMOVSD -0x18(%RDX),%XMM18                   | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVBROADCASTSD %XMM2,%YMM3                    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 5       | 1\nVMOVSD -0x10(%RDX),%XMM16                   | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVBROADCASTSD %XMM18,%ZMM11                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 5       | 1\nVMOVSD -0x8(%RDX),%XMM14                    | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVBROADCASTSD %XMM16,%ZMM7                   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 5       | 1\nVMOVSD (%RDX),%XMM1                         | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVBROADCASTSD %XMM14,%YMM4                   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 5       | 1\nVBROADCASTSD %XMM1,%YMM9                    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 5       | 1\nVFMADD231PD (%RBX,%RSI,1),%YMM13,%YMM0      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132SD 0x50(%RBX,%RDI,1),%XMM8,%XMM22  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD231PD 0x28(%RBX,%RSI,1),%YMM12,%YMM0  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132SD 0x78(%RBX,%RDI,1),%XMM22,%XMM19 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD231PD 0x28(%RCX,%RSI,1),%YMM11,%YMM0  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132SD 0x78(%RCX,%RDI,1),%XMM19,%XMM18 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD231PD 0x50(%RBX,%RSI,1),%YMM10,%YMM0  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132SD 0xa0(%RBX,%RDI,1),%XMM18,%XMM17 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD231PD 0x50(%RCX,%RSI,1),%YMM7,%YMM0   | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132SD 0xa0(%RCX,%RDI,1),%XMM17,%XMM16 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD231PD 0x78(%RBX,%RSI,1),%YMM5,%YMM0   | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132SD 0xc8(%RBX,%RDI,1),%XMM16,%XMM15 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD231PD 0x78(%RCX,%RSI,1),%YMM4,%YMM0   | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132SD 0xc8(%RCX,%RDI,1),%XMM15,%XMM14 | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132PD 0xa0(%RBX,%RSI,1),%YMM0,%YMM3   | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132SD 0xf0(%RBX,%RDI,1),%XMM14,%XMM2  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132PD 0xa0(%RCX,%RSI,1),%YMM3,%YMM9   | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFMADD132SD 0xf0(%RCX,%RDI,1),%XMM2,%XMM1   | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213PD 0x28(%RDX,%R10,8),%YMM6,%YMM9  | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213SD 0x28(%RDX),%XMM21,%XMM1        | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVMOVSD -0xc8(%RAX),%XMM22                   | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVDIVSD %XMM22,%XMM20,%XMM15                 | 3     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 40-42   | 12-32\nVMOVAPD %ZMM9,%ZMM25                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVSD -0xa0(%RAX),%XMM26                   | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVMOVAPD %ZMM1,%ZMM29                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVSD -0x78(%RAX),%XMM24                   | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nCMP %R9,%R11                                | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nVUNPCKHPD %XMM9,%XMM9,%XMM1                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nLEA 0x28(%RDX),%RDX                         | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nVEXTRACTF128 $0x1,%YMM9,%XMM12              | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 3-6     | 1\nLEA 0xc8(%RDI),%RDI                         | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nVMOVSD -0x50(%RAX),%XMM23                   | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nLEA 0xc8(%RSI),%RSI                         | 1     | 0    | 0    | 0    | 0    | 0.50 | 0.50 | 0  | 1       | 0.50\nVMOVSD -0x28(%RAX),%XMM19                   | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 0  | 5       | 0.50\nVMULSD -0xc0(%RAX),%XMM15,%XMM10            | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVMULSD -0xb8(%RAX),%XMM15,%XMM16            | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVMULSD -0xb0(%RAX),%XMM15,%XMM13            | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVMOVAPD %ZMM10,%ZMM27                       | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVAPD %ZMM10,%ZMM18                       | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVFNMADD213SD -0x98(%RAX),%XMM26,%XMM27      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVDIVSD %XMM27,%XMM20,%XMM0                  | 3     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 40-42   | 12-32\nVMOVAPD %ZMM16,%ZMM28                       | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVAPD %ZMM16,%ZMM4                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVFNMADD213SD -0x90(%RAX),%XMM26,%XMM28      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213SD -0x70(%RAX),%XMM24,%XMM18      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVMOVSD %XMM13,%XMM13,%XMM14                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVFNMADD213SD -0x68(%RAX),%XMM24,%XMM4       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213SD -0x88(%RAX),%XMM26,%XMM14      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVMOVAPD %ZMM10,%ZMM17                       | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVSD %XMM10,%XMM10,%XMM7                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVMOVAPD %ZMM16,%ZMM30                       | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVFNMADD132SD %XMM9,%XMM1,%XMM10             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVMOVAPD %ZMM16,%ZMM5                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVFNMADD132SD %XMM9,%XMM12,%XMM16            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVUNPCKHPD %XMM12,%XMM12,%XMM9               | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 4-7     | 2\nVMOVSD %XMM13,%XMM13,%XMM11                 | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVMOVSD %XMM13,%XMM13,%XMM3                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVMOVSD %XMM13,%XMM13,%XMM2                  | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVFNMADD213SD -0x20(%RAX),%XMM19,%XMM7       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213SD -0x48(%RAX),%XMM23,%XMM17      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM13,%XMM25,%XMM9            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVMULSD -0xa8(%RAX),%XMM15,%XMM8             | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213SD -0x60(%RAX),%XMM24,%XMM11      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213SD -0x38(%RAX),%XMM23,%XMM3       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213SD -0x10(%RAX),%XMM19,%XMM2       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVMULSD %XMM0,%XMM28,%XMM28                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213SD -0x40(%RAX),%XMM23,%XMM30      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213SD -0x18(%RAX),%XMM19,%XMM5       | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVMOVSD %XMM8,%XMM8,%XMM15                   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVMOVSD %XMM8,%XMM8,%XMM12                   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVMOVAPD %ZMM8,%ZMM31                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVSD %XMM8,%XMM8,%XMM1                    | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVFNMADD231SD %XMM28,%XMM18,%XMM4            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD132SD %XMM25,%XMM29,%XMM8            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVMOVAPD %ZMM4,%ZMM29                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVFNMADD213SD -0x80(%RAX),%XMM26,%XMM15      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVMULSD %XMM0,%XMM14,%XMM4                   | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM28,%XMM17,%XMM30           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213SD -0x58(%RAX),%XMM24,%XMM12      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213SD -0x30(%RAX),%XMM23,%XMM31      | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVMULSD %XMM0,%XMM15,%XMM13                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD213SD -0x8(%RAX),%XMM19,%XMM1        | 1     | 0.50 | 0.50 | 0.50 | 0.50 | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM28,%XMM7,%XMM5             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM28,%XMM10,%XMM16           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM4,%XMM7,%XMM2              | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM4,%XMM18,%XMM11            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM4,%XMM17,%XMM3             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD132SD %XMM10,%XMM9,%XMM4             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVDIVSD %XMM29,%XMM20,%XMM9                  | 3     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 40-42   | 12-32\nVFNMADD231SD %XMM13,%XMM7,%XMM1             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM13,%XMM18,%XMM12           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM13,%XMM17,%XMM31           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD132SD %XMM10,%XMM8,%XMM13            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVMOVAPD %ZMM31,%ZMM8                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVSD %XMM1,%XMM1,%XMM14                   | 1     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 2       | 1\nVMULSD %XMM9,%XMM11,%XMM11                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVMULSD %XMM9,%XMM12,%XMM15                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM11,%XMM30,%XMM3            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVDIVSD %XMM3,%XMM20,%XMM31                  | 3     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 40-42   | 12-32\nVFNMADD231SD %XMM11,%XMM5,%XMM2             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD132SD %XMM16,%XMM4,%XMM11            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM15,%XMM30,%XMM8            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM15,%XMM5,%XMM14            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD132SD %XMM16,%XMM13,%XMM15           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVMULSD %XMM8,%XMM31,%XMM28                  | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM28,%XMM2,%XMM14            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM28,%XMM11,%XMM15           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVDIVSD %XMM14,%XMM15,%XMM12                 | 3     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 40-42   | 12-32\nVFNMADD231SD %XMM12,%XMM2,%XMM11            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVDIVSD %XMM3,%XMM11,%XMM1                   | 3     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 40-42   | 12-32\nVMOVAPD %ZMM30,%ZMM3                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVAPD %ZMM18,%ZMM2                        | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 2       | 0.50\nVMOVSD %XMM12,(%RDX)                        | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVMOVSD %XMM11,0x98(%RSP)                    | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVFNMADD132SD %XMM1,%XMM16,%XMM3             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM12,%XMM5,%XMM3             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVDIVSD %XMM29,%XMM3,%XMM5                   | 3     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 40-42   | 12-32\nVMOVSD %XMM1,-0x8(%RDX)                     | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVMOVSD %XMM3,0x90(%RSP)                     | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVFNMADD132SD %XMM5,%XMM10,%XMM2             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM1,%XMM17,%XMM2             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD231SD %XMM12,%XMM7,%XMM2             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVDIVSD %XMM27,%XMM2,%XMM10                  | 3     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 40-42   | 12-32\nVMOVSD %XMM5,-0x10(%RDX)                    | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVMOVSD %XMM2,0x88(%RSP)                     | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVMOVSD %XMM10,-0x18(%RDX)                   | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVFNMADD132SD %XMM26,%XMM25,%XMM10           | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD132SD %XMM24,%XMM10,%XMM5            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD132SD %XMM23,%XMM5,%XMM1             | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVFNMADD132SD %XMM19,%XMM1,%XMM12            | 1     | 0.50 | 0.50 | 0    | 0    | 0    | 0    | 0  | 6       | 0.50\nVDIVSD %XMM22,%XMM12,%XMM7                  | 3     | 1    | 0    | 0    | 0    | 0    | 0    | 0  | 40-42   | 12-32\nVMOVSD %XMM12,0x80(%RSP)                    | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nVMOVSD %XMM7,-0x20(%RDX)                    | 1     | 0    | 0    | 0.50 | 0.50 | 0    | 0    | 1  | 2       | 1\nJNE 25fb0 <blts_+0x500>                     | 1     | 0    | 0    | 0    | 0    | 0    | 1    | 0  | 0       | 1-2\n",
        },
      },
      header = {
        "2% of peak computational performance is used (0.78 out of 32.00 FLOP per cycle (GFLOPS @ 1GHz))",
      },
      brief = {
      },
      gain = {
        {
          workaround = " - Try another compiler or update/tune your current one:\n  * recompile with fassociative-math (included in Ofast or ffast-math) to extend loop vectorization to FP reductions.\n - Remove inter-iterations dependences from your loop and make it unit-stride:\n  * If your arrays have 2 or more dimensions, check whether elements are accessed contiguously and, otherwise, try to permute loops accordingly:\nFortran storage order is column-major: do i do j a(i,j) = b(i,j) (slow, non stride 1) => do i do j a(j,i) = b(i,j) (fast, stride 1)\n  * If your loop streams arrays of structures (AoS), try to use structures of arrays instead (SoA):\ndo i a(i)%x = b(i)%x (slow, non stride 1) => do i a%x(i) = b%x(i) (fast, stride 1)\n",
          details = "17% of SSE/AVX instructions are used in vector version (process two or more data elements in vector registers):\n - 19% of SSE/AVX loads are used in vector version.\n - 0% of SSE/AVX stores are used in vector version.\n - 8% of SSE/AVX multiply instructions are used in vector version.\n - 14% of SSE/AVX fused multiply-add instructions are used in vector version.\n - 0% of SSE/AVX nil are used in vector version.\n - 41% of SSE/AVX instructions that are not load, store, addition, subtraction nor multiply instructions are used in vector version.\nSince your execution units are vector units, only a fully vectorized loop can use their full power.\n",
          title = "Vectorization",
          txt = "Your loop is not vectorized.\nOnly 23% of vector register length is used (average across all SSE/AVX instructions).\nBy fully vectorizing your loop, you can lower the cost of an iteration from 288.00 to 36.00 cycles (8.00x speedup).",
        },
        {
          workaround = " - Reduce the number of division or square root instructions:\n  * If denominator is constant over iterations, use reciprocal (replace x/y with x*(1/y)). Check precision impact. This will be done by your compiler with ffast-math or Ofast\n - Check whether you really need double precision. If not, switch to single precision to speedup execution\n",
          title = "Execution units bottlenecks",
          txt = "Performance is limited by execution of divide and square root operations (the divide/square root unit is a bottleneck).\n\nBy removing all these bottlenecks, you can lower the cost of an iteration from 288.00 to 79.00 cycles (3.65x speedup).\n",
        },
      },
      potential = {
        {
          title = "Expensive FP math instructions/calls",
          txt = "Detected performance impact from expensive FP math instructions/calls.\nBy removing/reexpressing them, you can lower the cost of an iteration from 288.00 to 79.00 cycles (3.65x speedup).",
        },
        {
          title = "FMA",
          txt = "Detected 100 FMA (fused multiply-add) operations.",
        },
      },
    },
  common = {
    header = {
      "The loop is defined in /users/user2210/NPB3.4-MZ-MPI/LU-MZ/blts.f90:57,70,80-195,201-219,225-227.\n",
      "The related source loop is not unrolled or unrolled with no peel/tail loop.",
    },
    nb_paths = 1,
  },
}
